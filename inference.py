#!/usr/bin/env python
"""
Interactive distributed inference for DeepSeek-Coder.

Ð—Ð°Ð¿ÑƒÑÐºÐ°Ð¹Ñ‚Ðµ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ñ‡ÐµÑ€ÐµÐ· torchrun / torch.distributed, Ñ‡Ñ‚Ð¾Ð±Ñ‹
ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð½Ð° ÑÐ²Ð¾Ñ‘Ð¼ GPU (Ñƒ Ð½Ð°Ñ â€• 1 GPU Ð½Ð° ÑƒÐ·ÐµÐ»).

Rank-0 (MASTER) Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¸Ð³Ð»Ð°ÑˆÐµÐ½Ð¸Ðµ >>>, ÐºÑƒÐ´Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ
Ð²Ð²Ð¾Ð´Ð¸Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ (ÐºÐ¾Ð´ Ð¸Ð»Ð¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ·Ñ‹Ðº). Â«EnterÂ» Ð½Ð° Ð¿ÑƒÑÑ‚Ð¾Ð¹
ÑÑ‚Ñ€Ð¾ÐºÐµ Ð·Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÑ‚ Ð²Ð²Ð¾Ð´.  ÐšÐ¾Ð¼Ð°Ð½Ð´Ñ‹  exit / quit  Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÑŽÑ‚
Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð².

ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€Ð°Ð½Ð³Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÑŽÑ‚ Ñ‚Ð¾Ñ‚ Ð¶Ðµ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ‡ÐµÑ€ÐµÐ· broadcast, Ð´ÐµÐ»Ð°ÑŽÑ‚
Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ Ð½Ð° ÑÐ²Ð¾Ð¸Ñ… GPU, Ð½Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ rank-0,
Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÑÑƒÑ€ÑÑ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð±ÐµÐ· ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð¾Ð².
"""

import os
import sys
import argparse
import torch
import torch.distributed as dist
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Distributed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dist.init_process_group(backend="nccl")
local_rank = int(os.getenv("LOCAL_RANK", 0))
torch.cuda.set_device(local_rank)
rank = dist.get_rank()
world_size = dist.get_world_size()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="DeepSeek-Coder interactive REPL")
parser.add_argument(
    "--model_path",
    default="/opt/models/deepseek-coder-6.7b-base",
    help="ÐŸÑƒÑ‚ÑŒ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð»Ð¸ ÐµÑ‘ repo-id Ð½Ð° HuggingFace",
)
parser.add_argument("--max_new_tokens", type=int, default=256,
                    help="Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð½Ð¾Ð²Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ")
parser.add_argument("--temperature", type=float, default=0.2,
                    help="Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° ÑÐµÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (0 = Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ð·Ð¼)")
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if rank == 0:
    print(f"[RANK 0] Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Â«{args.model_path}Â» â€¦", flush=True)

tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    args.model_path,
    torch_dtype=torch.float16,
    device_map={"": local_rank},        # Ð¾Ð´Ð¸Ð½ GPU Ð½Ð° Ð¿Ñ€Ð¾Ñ†ÐµÑÑ
    trust_remote_code=True,
).eval()

def bcast(obj, src=0):
    "Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ broadcast Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð»ÑŒÐ½Ñ‹Ñ… python-Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²"
    buf = [obj]
    dist.broadcast_object_list(buf, src)
    return buf[0]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. REPL-Ñ†Ð¸ÐºÐ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if rank == 0:
    print("\nDeepSeek-Coder Ð³Ð¾Ñ‚Ð¾Ð². Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð·Ð°Ð¿Ñ€Ð¾Ñ (ÐºÐ¾Ð´/Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑŽ)."
          "\nÐŸÑƒÑÑ‚Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ñ‚ Ð²Ð²Ð¾Ð´.  exit / quit â€” Ð²Ñ‹Ñ…Ð¾Ð´.\n")

while True:
    # -------- rank-0 Ñ‡Ð¸Ñ‚Ð°ÐµÑ‚ Ð²Ð²Ð¾Ð´ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ --------------------------------
    if rank == 0:
        user_lines = []
        sys.stdout.write(">>> ")
        sys.stdout.flush()
        for line in sys.stdin:
            line = line.rstrip("\n")
            if line == "":
                break
            user_lines.append(line)
            sys.stdout.write("... ")
            sys.stdout.flush()
        prompt = "\n".join(user_lines).strip()
        if prompt.lower() in {"exit", "quit"} or prompt == "":
            prompt = None
    else:
        prompt = None

    # -------- Ñ€Ð°ÑÑÑ‹Ð»ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð²ÑÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼ --------------------------------
    prompt = bcast(prompt, src=0)
    if prompt is None:
        if rank == 0:
            print("[RANK 0] Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ â€¦")
        break

    # -------- Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° ------------------------------------------------
    inputs = tokenizer(prompt, return_tensors="pt").to(local_rank)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            do_sample=args.temperature > 0,
            eos_token_id=tokenizer.eos_token_id,
        )

    # -------- Ð²Ñ‹Ð²Ð¾Ð´ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ rank-0) ------------------------------------------
    if rank == 0:
        full_text = tokenizer.decode(output[0], skip_special_tokens=True)
        completion = full_text[len(prompt):].lstrip("\n")
        print("\n=== ðŸ§  DeepSeek-Coder Ð¾Ñ‚Ð²ÐµÑ‚ ===\n" + completion + "\n")
